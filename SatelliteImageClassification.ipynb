{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "hide_code=true; \n",
       "function code_toggle() {\n",
       "   if (hide_code){\n",
       "       $('div.input').hide();\n",
       "   } else {\n",
       "       $('div.input').show();\n",
       "   }\n",
       "   hide_code = !hide_code\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "NOTE: the code in this notebook is hidden for better readability.  \n",
       "To toggle on/off, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "hide_code=true; \n",
    "function code_toggle() {\n",
    "   if (hide_code){\n",
    "       $('div.input').hide();\n",
    "   } else {\n",
    "       $('div.input').show();\n",
    "   }\n",
    "   hide_code = !hide_code\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "NOTE: the code in this notebook is hidden for better readability.  \n",
    "To toggle on/off, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Proposal – Data Science Practicum I </center>\n",
    "## <center>Dstl Satellite Imagery Feature Detection Problem: </center>\n",
    "\n",
    "\n",
    "My work and educational background is in environmental sciences and I have experience in GIS and remote sensing.  Given my background, I want to do a visualization problem that I can potentially use as an example for potential employers.  On kaggle, I found the Dstl Satellite Imagery Feature Detection Problem that I would like to tackle.  Several 1km x 1km Satellite Images have been supplied and the objective is to detect and classify the following objects found on the satellite images:\n",
    "\n",
    "* Buildings – large buildings, residential, non-residential, fuel storage facilities, fortified buildings\n",
    "* Misc. Man-made structures\n",
    "* Roads\n",
    "* Tracks – poor/dirt/cart tracks, footpaths/trails\n",
    "* Trees – woodlands, hedgerows, groups of trees, standalone trees\n",
    "* Crops – contour plowing/croplands, grain (wheat) crops, row (potatoes, turnips) crops\n",
    "* Waterways\n",
    "* Standing water\n",
    "* Large Vehicle – large vehicles (e.g. lorry, trucks, buses), logistics vehicles\n",
    "* Small Vehicle – cars, vans, motorbikes\n",
    "\n",
    "A discription of the problem can be found here:  www.kaggle.com/c/dstl-satellite-imagery-feature-detection\n",
    "Using Python, I will first use different types of classification methods such as SVM and k-mean to detect the objects.  I will compare the classifications and determine which method worked best. The classifications will be performed on each image-pixel this means that features such as object shapes, size and texture will not be accounted for when analyzing the satellite images.  To account for such features, it might be better to use deep learning.  Therefore if time permits a classification model will be performed using deep learning to determine if better results will be obtained using this method.\n",
    "\n",
    "### DATA:\n",
    "\n",
    "The Dstl Satellite Imagery Feature Detection Problem supplies you with 1km x 1km satellite images of 360 different locations taken using the WorldView-3 Satellite.  WorldView-3 takes multispectral and Panchromatic images of a region .  Multispectral images capture image data at specific frequencies across the electromagnetic spectrum.  Panchromatic images, on the other hand, are images that are sensitive to a wide range of wavelengths of light.\n",
    "\n",
    "![](worldview-3-spectral-bands.jpg?raw=true)\n",
    "\n",
    "Two types of imagery spectral content are provided, 3-band and 16-band images.  The 3-band consist of the RGB color bands with a resolution of ~0.3m.  The 16 band images consist of 8-Multispectral band images with a resolution of 1.24m, 8-SWIR (Short Wave Infrared) band images with a delivered resolution of ~7.5m and Panchromatic images with a resolution of 0.31m.  Details about the images are given in the table below:\n",
    "\n",
    "\n",
    "|**8 Multispectral band – resolution 1.24m**||\n",
    "|---------------------------------------|\n",
    "|Coastal:|                  400 - 450 nm|\n",
    "|Blue: |450 - 510 nm|\n",
    "|Green: |510 - 580 nm|\n",
    "|Yellow: |585 - 625 nm|\n",
    "|Red: |630 - 690 nm|\n",
    "|Red Edge: |705 - 745 nm|\n",
    "|Near-IR1: |770 - 895 nm|\n",
    "|Near-IR2: |860 - 1040 nm|\n",
    "|**8 SWIR - delivered resolution ~7.5m**||\n",
    "|SWIR-1: |1195 - 1225 nm|\n",
    "|SWIR-2: |1550 -1590 nm|\n",
    "|SWIR-3: |1640 -1680 nm|\n",
    "|SWIR-4: |1710 -1750 nm|\n",
    "|SWIR-5: |2145 - 2185 nm|\n",
    "|SWIR-6: |2185 - 2225 nm|\n",
    "|SWIR-7: |2235 - 2285 nm|\n",
    "|SWIR-8: |2295 - 2365 nm|\n",
    "|**Panchromatic – resolution 0.31m**||\n",
    "|Panchromatic: |450 - 800 nm|\n",
    "\n",
    "In addition to satellite images, training data for 25 of the image locations is provided.  The training data are georeferenced polygon files of classified objects found in the 25 different images.\n",
    "\n",
    "### DATA SOURCE:\n",
    "The data for this problem can be downloaded at:\n",
    "www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data\n",
    "\n",
    "|File Name|Available Formats|\n",
    "|----------------|\n",
    "|sample_submission.csv|.zip (14.89 kb)|\n",
    "|grid_sizes.csv|.zip (2.17 kb)|\n",
    "|sixteen_band|.zip (7.30 gb)|\n",
    "|three_band|.zip (12.87 gb)|\n",
    "|train_geojson_v3|.zip (14.22 mb)|\n",
    "|train_wkt_v4.csv|.zip (11.08 mb)|\n",
    "\n",
    "### ANALYZES\n",
    "\n",
    "The supplied WorldView-3 satellite images contains multispectral bands.  These bands contain data that might not be visible to the human eye.  For example, looking at the RGB image below-left we see a football field with grass.  However when examaning the image using near-Infrared we can determine that the field is actually made of artificial turf.\n",
    "\n",
    "![](9788139_orig.png?raw=true)\n",
    "\n",
    "As all the bands give different information, we want to use the information off all these bands to try and classify objects found in the Satellite images.  However, the 3-band data that was supplied is pan-sharpened data created from the Panchromatic Images and the RGB Layers Bands.  I will therefore not use this data for the analysis only use the 16-band data that was given.\n",
    "The three different types of images all have a different resolution.  For data to be analyzed using traditional classification methods, The images would have to be resampled to match resolutions.  In order to focus on the analysis part of the project, I will first solely look at the 8-Multispectral image.  When this Analysis is complete, I will go back, resample the images to matching resolutions and perform the analysis with 17 features (8-Multispectral images, 8-SWIR images, Panchromatic Images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import ogr\n",
    "import pickle\n",
    "from osgeo import gdal, osr, gdalconst\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Week 1 - Jan 7:***\n",
    "The first step of the analyzes using python is to convert the 25 satellite images that have test data to a data matrix that contains row, column and band data for each pixel.  I will first import the images (GeoTiffs format) using Gdal and then convert the images to a Numpy array.  I will then reshape the Numpy arrays to a data matrix and concatanate all the band data so that everything is contained in one matrix\n",
    "The next step will be to import the polygon files (Geojson format) and to rasterize the files.  Both processes will be done using Gdal.  After the files have been rasterized, they will be converted to Numpy arrays, reshaped to a data matrix. \n",
    "\n",
    "* only analyzing pixels therefore polygon files needs to get rasterize data needs to be re-formatted into a data matrix where the rows in the matrix contains all data for an individual pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#----------------FUNCTION: Import satellite images----------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def importsatimage(imagename):\n",
    "    \n",
    "    imagenameM = imagename + \"_M.tif\"    \n",
    "    #print imagenameM\n",
    "    satM = gdal.OpenShared(os.path.join('sixteen_band/',imagenameM),gdalconst.GA_Update)\n",
    "    \n",
    "        # Open the data source and read in the extent\n",
    "    \n",
    "    if os.path.isfile('sixteen_band/'+imagenameM):\n",
    "        satM = gdal.OpenShared(os.path.join('sixteen_band/',imagenameM),gdalconst.GA_Update)\n",
    "    else:\n",
    "        raise IOError('Could not find file ' + imagenameM)\n",
    "    if satM is None:\n",
    "        raise IOError('Could open file ' + str(satM))\n",
    "        \n",
    "    return satM\n",
    "\n",
    "#---------------FUNCTION: Convert Geotiff files to numpy arrays, resize the arrays same size---------------------------#\n",
    "\n",
    "def geotiff2numpy(sat, dir):\n",
    "\n",
    "    L1 = np.array(sat.GetRasterBand(1).ReadAsArray(),dtype=np.int16)\n",
    "    L2 = np.array(sat.GetRasterBand(2).ReadAsArray(),dtype=np.int16)\n",
    "    L3 = np.array(sat.GetRasterBand(3).ReadAsArray(),dtype=np.int16)\n",
    "    L4 = np.array(sat.GetRasterBand(4).ReadAsArray(),dtype=np.int16)\n",
    "    L5 = np.array(sat.GetRasterBand(5).ReadAsArray(),dtype=np.int16)\n",
    "    L6 = np.array(sat.GetRasterBand(6).ReadAsArray(),dtype=np.int16)\n",
    "    L7 = np.array(sat.GetRasterBand(7).ReadAsArray(),dtype=np.int16)\n",
    "    L8 = np.array(sat.GetRasterBand(8).ReadAsArray(),dtype=np.int16)\n",
    " \n",
    "    #convert bands in XYZ format\n",
    "    \n",
    "    row,col = L1.shape\n",
    "    R,C = np.mgrid[:row,:col]\n",
    "    \n",
    "    name = np.full((row*col, 1), dir, dtype=np.dtype('a8'))\n",
    "    nameCR = np.column_stack((name, C.ravel(),R.ravel()))\n",
    "\n",
    "    XYLbands = np.column_stack((L1.ravel(),L2.ravel(),L3.ravel(),L4.ravel(),L5.ravel(),L6.ravel(),L7.ravel(),L8.ravel() ))\n",
    "\n",
    "    return XYLbands, nameCR\n",
    "\n",
    "#-----------------FUNCTION: Import polygon files and reproject them----------------------------------------------------#\n",
    "\n",
    "def importpolygons(dir, file, M_col, M_row, x_max, y_min):\n",
    "    \n",
    "    polygonfilename = 'train_geojson_v3/' + dir + '/' + file\n",
    "\n",
    "    # Define pixel_size and NoData value of new raster\n",
    "\n",
    "    NoData_value = 255\n",
    "\n",
    "    # Open the data source and read in the extent\n",
    "    \n",
    "    polygonfile = ogr.Open(polygonfilename)\n",
    "    \n",
    "    if os.path.isfile(polygonfilename):\n",
    "        polygon = ogr.Open(polygonfilename)\n",
    "    else:\n",
    "        raise IOError('Could not find file ' + polygonfilename)\n",
    "    if polygon is None:\n",
    "        raise IOError('Could open file ' + str(polygon))\n",
    "    \n",
    "    polygon_layer = polygon.GetLayer()\n",
    "    polygon_srs = polygon_layer.GetSpatialRef()\n",
    "    x_min = 0\n",
    "    y_max = 0\n",
    "    \n",
    "    if x_max == 999:\n",
    "        x_min, x_max, y_min, y_max = polygon_layer.GetExtent()\n",
    "    pixel_sizex = x_max/M_col\n",
    "    pixel_sizey = y_min/M_row\n",
    "\n",
    "    # Create the destination data source\n",
    "    \n",
    "    x_res = int((x_max - x_min) / pixel_sizex)\n",
    "    y_res = -1*int((y_max - y_min) / pixel_sizey)\n",
    "    target_ds = gdal.GetDriverByName('MEM').Create('', x_res, y_res, gdal.GDT_Byte)\n",
    "    target_ds.SetGeoTransform((0, pixel_sizex, 0, 0, 0, pixel_sizey))\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(NoData_value)\n",
    "\n",
    "    # Rasterize the Polygon file\n",
    "    \n",
    "    gdal.RasterizeLayer(target_ds, [1], polygon_layer, burn_values=[1])\n",
    "    \n",
    "    array = band.ReadAsArray()\n",
    "    classified = array.ravel()\n",
    "    \n",
    "    return classified, x_max, y_min\n",
    "\n",
    "#-----------------FUNCTION: Assign an ID to the polygon file names-----------------------------------------------------#\n",
    "\n",
    "def referenceID():\n",
    "\n",
    "    reference = {}\n",
    "    with open('TRAINING_REFERENCE.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            reference[row['NAME']] = int(row['UNIQUEID'])\n",
    "\n",
    "    return reference\n",
    "\n",
    "#-----------------FUNCTION: Read all vegetation polygon files and convert them to a data matrix------------------------#\n",
    "\n",
    "def vegetation_classifications(parentdir,dir, reference, M_col, M_row):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for _, _, files in os.walk(os.path.join(parentdir, dir)):\n",
    "        \n",
    "        files.sort(reverse=True)\n",
    "\n",
    "        class_matrix = np.zeros((M_col*M_row,5),dtype=np.int16)\n",
    "    \n",
    "        for file in files:              \n",
    "                                \n",
    "            if 'Grid' in file:\n",
    "\n",
    "                gridarray, x_max, y_min = importpolygons(dir, file, M_col, M_row, 999, -999)\n",
    "\n",
    "            elif 'VEG' in file:\n",
    "                    \n",
    "                ID = reference[file]-18\n",
    "                class_matrix[:,ID], x, y = importpolygons(dir, file, M_col, M_row, x_max, y_min)\n",
    "            \n",
    "    return class_matrix \n",
    "\n",
    "\n",
    "#-----------------FUNCTION: Convert Data-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------FUNCTION: Read in data it to a data matrix format----------------------------------------------------#\n",
    "\n",
    "def dataintomatrix():\n",
    "    \n",
    "    data_matrix = None\n",
    "    vegetation_matrix = None\n",
    "    satsize = {}\n",
    "\n",
    "\n",
    "    reference = referenceID()\n",
    "    \n",
    "# read which images have test data\n",
    "\n",
    "    for parentdir, dirs, _ in os.walk('./train_geojson_v3/'):\n",
    "\n",
    "# read in all satellite images that have test data and store the pixels in a data_matrix\n",
    "        for dir in dirs:\n",
    "        \n",
    "            ImgM = importsatimage(dir)            \n",
    "            M_row = ImgM.RasterYSize\n",
    "            M_col = ImgM.RasterXSize\n",
    "            satsize[dir] = M_row, M_col\n",
    "        \n",
    "# Create a data matrix with satellite band information for each pixel        \n",
    "\n",
    "            if data_matrix is None:\n",
    " \n",
    "                data_matrix, nameCR = geotiff2numpy(ImgM, dir)\n",
    "    \n",
    "            else:\n",
    "\n",
    "                add2datamatrix, add2name = geotiff2numpy(ImgM, dir)\n",
    "                data_matrix = np.concatenate((data_matrix, add2datamatrix),axis=0)\n",
    "                nameCR_matrix = np.concatenate((nameCR, add2name),axis=0)\n",
    "\n",
    "# Create a vegetation data matrix for each pixel  \n",
    "\n",
    "            veg_matrix = vegetation_classifications(parentdir,dir, reference, M_col, M_row)   \n",
    "\n",
    "            if vegetation_matrix is None:\n",
    " \n",
    "                vegetation_matrix = veg_matrix\n",
    "    \n",
    "            else:\n",
    "\n",
    "                vegetation_matrix = np.concatenate((vegetation_matrix, veg_matrix),axis=0)\n",
    "                \n",
    "#pickle up the data:\n",
    "\n",
    "    file = open('data_matrix.p', 'w')\n",
    "    pickle.dump(data_matrix, file)\n",
    "    file.close()\n",
    "\n",
    "    file = open('vegetation_matrix.p', 'w')\n",
    "    pickle.dump(vegetation_matrix, file)\n",
    "    file.close()\n",
    "    \n",
    "    file = open('nameCR_matrix.p', 'w')\n",
    "    pickle.dump(nameCR_matrix, file)\n",
    "    file.close()\n",
    "    \n",
    "    return \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "dataintomatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### <center>**Pre-processing**</center>\n",
    "* Standardize data\n",
    "$$x_i=\\frac{x -\\mu}{\\sigma}$$\n",
    "\n",
    "$\\mu$is the mean $\\sigma$ is the standard deviation....\n",
    "* NDVI filter\n",
    "$$NDVI=\\frac{NIR -RED}{NIR+RED}$$\n",
    "*EVI filter\n",
    "$$NDVI=\\frac{NIR -RED}{NIR+C1\\cdot RED+C2\\cdot BLUE+L}$$\n",
    "\n",
    "*\"where NIR/red/blue are atmospherically-corrected or partially atmosphere corrected (Rayleigh and ozone absorption) surface reflectances, L is the canopy background adjustment that addresses non-linear, differential NIR and red radiant transfer through a canopy, and C1, C2 are the coefficients of the aerosol resistance term, which uses the blue band to correct for aerosol influences in the red band. The coefficients adopted in the MODIS-EVI algorithm are; L=1, C1 = 6, C2 = 7.5, and G (gain factor) = 2.5.\"*\n",
    "\n",
    "worldview 3 sensors: http://www.indexdatabase.de/db/s-single.php?id=46\n",
    "\n",
    "\n",
    "\n",
    "***Weeks 2 & 3 – Jan 21:***\n",
    "After the data has been imported and formatted, we will use sklearn to analyze the data.  The data will first be split into a training and testing data set.  The training data tset will be used to create classification models using sklearn.  After we have created the models we will cross-validate them using the testing data, and I will calculate the training and test error.  For each analyses a model , PR curves, ROC curves, and confusion matrixes will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addNDVI(datamatrix):\n",
    "    datamatrix = datamatrix.astype(np.float64)\n",
    "    NDVI = np.matrix((datamatrix[:,7]-datamatrix[:,4])/(datamatrix[:,7]+datamatrix[:,4])).T\n",
    "    datamatrix_NDVI = np.concatenate((datamatrix,NDVI),axis=1)\n",
    "\n",
    "    return datamatrix_NDVI\n",
    "\n",
    "def standardizedata(datamatrix):\n",
    "    r,c = datamatrix.shape\n",
    "    for n in range(0,c):\n",
    "        datamatrix[:,n] = preprocessing.scale(datamatrix[:,n])\n",
    "    datamatrix = datamatrix.astype(np.float16)\n",
    "    return datamatrix\n",
    "\n",
    "\n",
    "\n",
    "def preprocessingdata():\n",
    "    file = open('data_matrix.p', 'r')\n",
    "    datamatrix = pickle.load(file)\n",
    "    file.close()\n",
    "    datamatrix_NDVI = addNDVI(datamatrix)\n",
    "    standardized_matrix = standardizedata(datamatrix_NDVI)\n",
    "    \n",
    "    file = open('standardized_matrix.p', 'w')\n",
    "    pickle.dump(standardized_matrix, file)\n",
    "    file.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "preprocessingdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* address imbalanced classes: http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------FUNCTION: Convert Data-------------------------------------------------------------------------------#\n",
    "\n",
    "def convertvegdata():\n",
    "\n",
    "    file = open('vegetation_matrix.p', 'r')\n",
    "    vegetation_matrix = pickle.load(file)\n",
    "    file.close()\n",
    "#\n",
    "    num_rows, num_col = vegetation_matrix.shape\n",
    "\n",
    "    y = np.empty([num_rows, 1],dtype=np.int16)\n",
    "    \n",
    "    for n in range(0,num_rows - 1):\n",
    "        if vegetation_matrix[n,4] == 1 or vegetation_matrix[n,3] == 1:\n",
    "            y[n]=1\n",
    "        elif vegetation_matrix[n,1] == 1:\n",
    "            y[n]=2\n",
    "        elif vegetation_matrix[n,0] == 1:\n",
    "            y[n]=3\n",
    "        else:\n",
    "            y[n]=0\n",
    "\n",
    "    return y\n",
    "\n",
    "def combinedatasets(targets):\n",
    "    \n",
    "    file = open('standardized_matrix.p', 'r')\n",
    "    standardized_matrix = pickle.load(file)\n",
    "    file.close()\n",
    "    alldata = np.concatenate((standardized_matrix,targets),axis=1)\n",
    "    alldata = alldata.astype(np.float16)\n",
    "    \n",
    "    return alldata\n",
    "\n",
    "def combinedata():\n",
    "\n",
    "    target = convertvegdata()\n",
    "    alldata = combinedatasets(target)\n",
    "\n",
    "    file = open('alldata.p', 'w')\n",
    "    pickle.dump(alldata, file)\n",
    "    file.close()\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "combinedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def woodland(alldata):\n",
    "    alldata = np.array(alldata)\n",
    "    woodland = alldata[alldata[:,9] == 2]\n",
    "    notwoodland = alldata[alldata[:,9] != 2]\n",
    "    file = open('woodland.p', 'w')\n",
    "    pickle.dump(woodland , file)\n",
    "    file.close()\n",
    "    file = open('notwoodland.p', 'w')\n",
    "    pickle.dump(notwoodland , file)\n",
    "    file.close()\n",
    "    return\n",
    "    \n",
    "def shrubland(alldata):\n",
    "    alldata = np.array(alldata)\n",
    "    shrubland = alldata[alldata[:,9] == 3]\n",
    "    notshrubland = alldata[alldata[:,9] != 3]\n",
    "    file = open('shrubland.p', 'w')\n",
    "    pickle.dump(shrubland, file)\n",
    "    file.close()\n",
    "    file = open('notshrubland.p', 'w')\n",
    "    pickle.dump(notshrubland, file)\n",
    "    file.close()\n",
    "    return\n",
    "def trees(alldata):\n",
    "    alldata = np.array(alldata)\n",
    "    trees = alldata[alldata[:,9] == 1]\n",
    "    nottrees = alldata[alldata[:,9] != 1]\n",
    "    file = open('trees.p', 'w')\n",
    "    pickle.dump(trees, file)\n",
    "    file.close()\n",
    "    file = open('nottrees.p', 'w')\n",
    "    pickle.dump(nottrees, file)\n",
    "    file.close()\n",
    "    return\n",
    "def other(alldata):\n",
    "    alldata = np.array(alldata)\n",
    "    other = alldata[alldata[:,9] == 0]\n",
    "    file = open('other.p', 'w')\n",
    "    pickle.dump(other, file)\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "def createvegtypefiles():\n",
    "    file = open('alldata.p', 'r')\n",
    "    alldata = pickle.load(file)\n",
    "    file.close()\n",
    "    woodland(alldata)\n",
    "    shrubland(alldata)\n",
    "    trees(alldata)\n",
    "    other(alldata)\n",
    "    return\n",
    "\n",
    "createvegtypefiles()\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('woodland.p', 'r')\n",
    "woodland = pickle.load(file)\n",
    "file.close()\n",
    "file = open('shrubland.p', 'r')\n",
    "shrubland = pickle.load(file)\n",
    "file.close()\n",
    "file = open('trees.p', 'r')\n",
    "trees = pickle.load(file)\n",
    "file.close()\n",
    "file = open('other.p', 'r')\n",
    "other = pickle.load(file)\n",
    "file.close()\n",
    "r,c = woodland.shape\n",
    "testpercent = 1 - 80000/float(r)\n",
    "wx_train, wx_test, wy_train, wy_test = train_test_split(woodland[:,0:9], woodland[:,9], test_size=testpercent, random_state=0)\n",
    "r,c = shrubland.shape\n",
    "testpercent = 1 - 80000/float(r)\n",
    "sx_train, sx_test, sy_train, sy_test = train_test_split(shrubland[:,0:9], shrubland[:,9], test_size=testpercent, random_state=0)\n",
    "r,c = trees.shape\n",
    "testpercent = 1 - 80000/float(r)\n",
    "tx_train, tx_test, ty_train, ty_test = train_test_split(trees[:,0:9], trees[:,9], test_size=testpercent, random_state=0)\n",
    "r,c = other.shape\n",
    "testpercent = 1 - 80000/float(r)\n",
    "ox_train, ox_test, oy_train, oy_test = train_test_split(other[:,0:9], other[:,9], test_size=testpercent, random_state=0)\n",
    "knnx_train = np.concatenate((wx_train,sx_train,tx_train,ox_train),axis=0)\n",
    "knny_train = np.concatenate((wy_train,sy_train,ty_train,oy_train),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#kmeans = KMeans(n_clusters=20, max_iter=500, random_state=0).fit(x_train)\n",
    "#y = kmeans.labels_\n",
    "#y_pred = kmeans.predict(x_train)\n",
    "#conf = confusion_matrix(y, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f1=open('./testfile.txt', 'w')\n",
    "#np.set_printoptions(threshold='nan')\n",
    "#print >> f1, conf\n",
    "#f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(knnx_train, knny_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = knn.predict(knnx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confknn = confusion_matrix(y, knny_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69931  6326   248  3105]\n",
      " [ 5097 64652   240  7243]\n",
      " [  393   857 76784  6186]\n",
      " [ 4579  8165  2727 63466]]\n"
     ]
    }
   ],
   "source": [
    "print confknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = knn.predict(x_test)\n",
    "confknntest = confusion_matrix(y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confknntest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#clf = SVC(cache_size=1000)\n",
    "#print clf\n",
    "#clf.fit(x_train, y_train) \n",
    "\n",
    "#y_pred = (clf.predict(x_train))\n",
    "\n",
    "#print confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_train2, x_test2, y_train2, y_test2 = train_test_split(x_test, y_test, test_size=0.001, random_state=0)\n",
    "#y_test_pred = (clf.predict(x_test2))\n",
    "#print confusion_matrix(y_test2, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Week 4 – Jan 28:***\n",
    "After the classification we will convert classified data into geotiff images and overly them onto the satellite images.\n",
    "\n",
    "***Week 5 – Feb 4:***\n",
    "Once the classification on the 8-Multispectrum images has been completed, I will resample the 8-Multispectral and 8-SWIR images to match the resolution of the Panchromatic Images.  I will then follow the steps laid out above and perform a classification on the 17 feature data set (8-Multispectral images, 8-SWIR images, Panchromatic Images) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
